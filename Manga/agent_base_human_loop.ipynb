{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1413c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver,InMemorySaver\n",
    "from langgraph.types import Command, interrupt\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing import TypedDict,Annotated,Literal,List,Optional\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n",
    "import json\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08f1a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import google.generativeai as genai\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# load_dotenv()\n",
    "# genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# print(\"Available models that support generateContent:\\n\")\n",
    "\n",
    "# for m in genai.list_models():\n",
    "#     methods = getattr(m, \"supported_generation_methods\", [])\n",
    "#     if \"generateContent\" in methods:\n",
    "#         print(m.name, methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "054ba94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Schema For Overall Features from the Story\n",
    "class MangaFeatureSchema(BaseModel):\n",
    "    main_characters: List[str] = Field(\n",
    "        ..., description=\"List of main characters in the story, including roles or names.\"\n",
    "    )\n",
    "    \n",
    "    character_descriptions: List[str] = Field(\n",
    "        ..., description=\"Short descriptions of the characters‚Äô traits, personalities, or roles.\"\n",
    "    )\n",
    "    \n",
    "    setting: str = Field(\n",
    "        ..., description=\"The primary setting or environment where the story takes place.\"\n",
    "    )\n",
    "    \n",
    "    conflict_or_goal: str = Field(\n",
    "        ..., description=\"The main conflict, tension, or goal driving the story.\"\n",
    "    )\n",
    "    \n",
    "    important_objects: List[str] = Field(\n",
    "        ..., description=\"Key objects, weapons, or magical items relevant to the story.\"\n",
    "    )\n",
    "    \n",
    "    mood_and_tone: List[str] = Field(\n",
    "        ..., description=\"Keywords describing the mood and tone of the story (normalized to lowercase).\"\n",
    " )\n",
    "    \n",
    "    key_sound_effects_and_emotions: List[str] = Field(\n",
    "        ..., description=\"Important sound effects (onomatopoeia) and strong emotions expressed in the story.\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Schema for Character description\n",
    "class CharacterProfile(BaseModel):\n",
    "    name_or_role: str                 # e.g. \"Curious Boy\"\n",
    "    canonical_name: Optional[str]     # e.g. \"Taro\" (or null)\n",
    "    age_range: str                    # e.g. \"early teens (13-15)\"\n",
    "    gender_presentation: Optional[str]# e.g. \"male-presenting\" or \"non-binary\"\n",
    "    body_type: str                    # e.g. \"slim, small frame\"\n",
    "    height: Optional[str]             # e.g. \"short\" or \"170 cm\"\n",
    "    face: str                         # short face description: shape, nose, mouth\n",
    "    hair: str                         # color, style, length\n",
    "    eyes: str                         # color, shape, notable features\n",
    "    clothing: str                     # typical outfit description\n",
    "    accessories: List[str]            # e.g. [\"rope belt\", \"necklace\"]\n",
    "    color_palette: List[str]          # hex or basic color names, ordered primary ‚Üí accent\n",
    "    notable_marks: List[str]          # scars, tattoos, birthmarks\n",
    "    important_objects: List[str]      # items tied to the character, can be []\n",
    "    signature_poses: List[str]        # short phrases e.g. [\"hand-on-hilt\", \"heroic stance\"]\n",
    "    default_expressions: List[str]    # e.g. [\"wide-eyed shock\",\"determined glare\"]\n",
    "    voice_short: Optional[str]        # quick tonal note for dialogue (e.g. \"soft, inquisitive\")\n",
    "    drawing_instructions: str         # manga-specific tips: line weight, shading, typical camera angle\n",
    "    visual_reference_prompt: str      # 1-2 sentence short prompt formatted for image models\n",
    "    consistency_token: str            # unique id you can pass to image-generator to keep same character\n",
    "\n",
    "### For list of characters\n",
    "class CharacterList(BaseModel):\n",
    "    characters: List[CharacterProfile] = Field(..., description=\"List of character profiles\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SceneFeature(BaseModel):\n",
    "    scene_number: int\n",
    "    summary: str                                # 1‚Äì2 sentence summary\n",
    "    setting_details: str                        # description of location, mood, time\n",
    "    characters_involved: List[str]              # must match character_makeup entries\n",
    "    actions: List[str]                          # short action phrases\n",
    "    emotions: List[str]                         # emotional keywords\n",
    "    potential_dialogues: List[str]              # \"Name: text\"\n",
    "    inner_thoughts: List[str]                   # [inner thought style]\n",
    "    sound_effects: List[str]                    # onomatopoeia list\n",
    "    visual_elements: List[str]                   # Key visual cues, colors, lighting, objects, symbols\n",
    "\n",
    "class SceneFeatureList(BaseModel):\n",
    "    scenes: List[SceneFeature]\n",
    "\n",
    "\n",
    "## Schema For Director\n",
    "class Director_Panel(BaseModel):\n",
    "    panel_number: int\n",
    "    scene_description: str              # What is shown in the panel (setting, action, camera)\n",
    "    characters_present: List[str]       # From character_setup\n",
    "    actions: List[str]                  # Key actions happening in this panel\n",
    "    dialogues: List[str]                # Short speech bubbles, \"Name: text\"\n",
    "    inner_thoughts: List[str]           # If any, written as [thoughts]\n",
    "    sound_effects: List[str]\n",
    "    visual_elements: List[str] \n",
    "    \n",
    "\n",
    "## Schema for Number of Pages\n",
    "\n",
    "class MangaPage(BaseModel):\n",
    "    page_number: int\n",
    "    panels: List[Director_Panel]\n",
    "\n",
    "\n",
    "\n",
    "# Schema for each generated image prompt per panel\n",
    "class MangaImagePrompt(BaseModel):\n",
    "    panel_number: int = Field(..., description=\"The panel number from the director script\")\n",
    "    image_prompt: str = Field(\n",
    "        ..., \n",
    "        description=\"Short, clear description of what the image generation model should draw, including characters (with consistency_token), setting, action, emotions, camera angle, and sound effects\"\n",
    "    )\n",
    "\n",
    "#  Schema for one manga page of generated prompts\n",
    "class MangaImagePromptPage(BaseModel):\n",
    "    page_number: int = Field(..., description=\"Page number in the manga\")\n",
    "    panel_prompts: List[MangaImagePrompt] = Field(\n",
    "        ..., description=\"List of image prompts corresponding to panels on this page\"\n",
    "    )\n",
    "\n",
    "# # Schema for multiple pages (if extend later)\n",
    "# class MangaImagePromptBook(BaseModel):\n",
    "#     pages: List[MangaImagePromptPage] = Field(\n",
    "#         ..., description=\"List of pages, each with its panel image prompts\"\n",
    "#     )\n",
    "\n",
    "# Add these schemas to your existing schema section (after MangaImagePromptPage)\n",
    "\n",
    "class PanelAnalysis(BaseModel):\n",
    "    panel_number: int = Field(..., description=\"Panel number being analyzed\")\n",
    "    quality_score: int = Field(..., ge=1, le=10, description=\"Quality score from 1-10, where 8+ is acceptable\")\n",
    "    issues: List[str] = Field(..., description=\"List of specific problems found in the prompt\")\n",
    "    strengths: List[str] = Field(..., description=\"List of good elements in the prompt\")\n",
    "    needs_improvement: bool = Field(..., description=\"Whether this panel needs improvement\")\n",
    "    suggested_fixes: List[str] = Field(..., description=\"Specific improvement suggestions\")\n",
    "\n",
    "class PromptAnalysisResult(BaseModel):\n",
    "    panel_analyses: List[PanelAnalysis] = Field(..., description=\"Analysis for each panel\")\n",
    "    overall_score: float = Field(..., description=\"Average quality score across all panels\")\n",
    "    total_panels: int = Field(..., description=\"Total number of panels analyzed\")\n",
    "    panels_needing_improvement: int = Field(..., description=\"Count of panels that need improvement\")\n",
    "    needs_regeneration: bool = Field(..., description=\"Whether prompts should be regenerated\")\n",
    "    generation_attempt: int = Field(..., description=\"Current generation attempt number\")\n",
    "    max_attempts_reached: bool = Field(..., description=\"Whether maximum attempts have been reached\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d74f1c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# model= ChatGoogleGenerativeAI(\n",
    "#     model=\"models/gemma-3-1b-it\",\n",
    "#     # temperature=0.2,\n",
    "#     # max_output_tokens=512\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13328b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "_: bool = load_dotenv(find_dotenv())\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "structured_model_MangaFeature=model.with_structured_output(MangaFeatureSchema)\n",
    "structured_model_characterList=model.with_structured_output(CharacterList)\n",
    "structured_model_director=model.with_structured_output(MangaPage)\n",
    "structured_model_scene=model.with_structured_output(SceneFeatureList)\n",
    "structured_model_Mangaprompt=model.with_structured_output(MangaImagePromptPage)\n",
    "structured_model_PromptAnalysis = model.with_structured_output(PromptAnalysisResult)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b5112cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MangaFeatureSchema(main_characters=['Kira', 'Ren'], character_descriptions=['A young prodigy with a mysterious past.', 'A seasoned warrior seeking redemption.'], setting='A futuristic city with towering skyscrapers and hidden underground labyrinths.', conflict_or_goal=\"Kira and Ren must unite to stop a rogue AI from enslaving humanity, while also uncovering the truth behind Kira's origins.\", important_objects=[\"The 'Aegis' - a legendary sword said to choose its wielder.\", \"The 'Chrono-Shard' - a device capable of manipulating time.\"], mood_and_tone=['epic', 'dark', 'hopeful'], key_sound_effects_and_emotions=['CLANG (of swords)', 'WHIZZ (of energy blasts)', 'HOPE (determination)', 'DESPAIR (overwhelming odds)'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_model_MangaFeature.invoke(\"Generate a manga story with the following features: \\n\\n\" + \"The main characters in the story are: \\n\\n\" + \"The setting is: \\n\\n\" + \"The conflict or goal is: \\n\\n\" + \"The important objects are: \\n\\n\" + \"The mood and tone is: \\n\\n\" + \"The key sound effects and emotions are: \\n\\n\" + \"Please provide the following features in the format of a JSON object with the following keys: \\n\\n\" + \"main_characters, character_descriptions, setting, conflict_or_goal, important_objects, mood_and_tone, key_sound_effects_and_emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "607b9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MangaState(TypedDict):\n",
    "    input_story:str\n",
    "    refined_story:str\n",
    "    reviewed_story: str\n",
    "    extracted_features:dict\n",
    "    character_feature:dict\n",
    "    scene_features:dict\n",
    "    panel_scenes:dict\n",
    "    manga_image_prompts:dict\n",
    "    prompt_analysis: dict  # field for analysis results\n",
    "    generation_attempts: int  # field to track loop count\n",
    "    max_attempts: int  # configurable max attempts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cab97ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_refinner(state:MangaState):\n",
    "    user_story=state['input_story']\n",
    "    prompt=f'''\n",
    "            You are a professional manga storyteller. \n",
    "            Your job is to take a short user query and refine it into a concise manga-style story \n",
    "            suitable for ONE PAGE comic (4‚Äì6 sentences only).\n",
    "\n",
    "            Requirements:\n",
    "            - Keep the story short and dynamic (not more than 6 sentences).\n",
    "            - Add manga-style elements: \n",
    "            * Dramatic emotions \n",
    "            * Exaggerated action or reactions \n",
    "            * Inner thoughts (marked with brackets [ ])\n",
    "            * Sound effects (onomatopoeia like \"BAM!\", \"WHOOSH!\", \"Gyaa!\")\n",
    "            - Story should feel like it can naturally be divided into 4‚Äì5 panels later.\n",
    "            - Do not write panel breakdowns yet.\n",
    "\n",
    "            User Query: {user_story}\n",
    "\n",
    "            Refined Manga Story:\n",
    "            '''\n",
    "    refine_output=model.invoke(prompt).content\n",
    "    print(refine_output)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    return {\"refined_story\": refine_output}\n",
    "\n",
    "def review_node(state: MangaState):\n",
    "    # Ask a reviewer to edit the generated content\n",
    "    updated = interrupt({\n",
    "        \"instruction\": \"Review and edit this content\",\n",
    "        \"content\": state[\"refined_story\"],\n",
    "    })\n",
    "    return {\"reviewed_story\": updated}\n",
    "\n",
    "\n",
    "# def human_review(state: MangaState):\n",
    "#     \"\"\"\n",
    "#     Human-in-the-loop: Shows refined story and asks user for approval or edits.\n",
    "#     \"\"\"\n",
    "#     refined_story = state['refined_story']\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"REFINED MANGA STORY:\")\n",
    "#     print(\"=\"*60)\n",
    "#     print(refined_story)\n",
    "#     print(\"=\"*60)\n",
    "    \n",
    "#     while True:\n",
    "#         user_input = input(\"\\nOptions:\\n1. Approve (press Enter or type 'approve')\\n2. Edit (type your changes)\\n3. Regenerate (type 'regenerate')\\n\\nYour choice: \").strip()\n",
    "        \n",
    "#         if user_input == \"\" or user_input.lower() == \"approve\":\n",
    "#             print(\"‚úì Story approved! Continuing to next step...\\n\")\n",
    "#             return {\"user_approved\": True, \"user_edits\": None}\n",
    "        \n",
    "#         elif user_input.lower() == \"regenerate\":\n",
    "#             print(\"üîÑ Regenerating story...\\n\")\n",
    "#             # Re-run prompt refiner with original story\n",
    "#             return {\"user_approved\": False, \"refined_story\": prompt_refinner(state)[\"refined_story\"]}\n",
    "        \n",
    "#         else:\n",
    "#             # User provided edits\n",
    "#             print(\" Changes saved! Continuing with your edited version...\\n\")\n",
    "#             print(\"\\n\" + \"=\"*60)\n",
    "#             print(\"YOUR EDITED VERSION:\")\n",
    "#             print(\"=\"*60)\n",
    "#             print(user_input)\n",
    "#             print(\"=\"*60)\n",
    "#             return {\"user_approved\": True, \"refined_story\": user_input, \"user_edits\": user_input}\n",
    "\n",
    "\n",
    "# def should_continue_to_features(state: MangaState) -> str:\n",
    "#     \"\"\"\n",
    "#     Router: decides whether to continue or loop back for regeneration.\n",
    "#     \"\"\"\n",
    "#     if state.get('user_approved', False):\n",
    "#         return \"continue\"\n",
    "#     else:\n",
    "#         return \"regenerate\"\n",
    "\n",
    "\n",
    "\n",
    "def feature_extractor(state:MangaState):\n",
    "    refine_story = state.get(\"reviewed_story\") or state[\"refined_story\"]\n",
    "\n",
    "    prompt=f\"\"\"\n",
    "\n",
    "            You are a manga story analyzer. \n",
    "            Your task is to read the following refined manga story and extract its key features. \n",
    "            You MUST return the result as valid JSON that conforms to the MangaFeatureSchema below:\n",
    "\n",
    "            Schema:\n",
    "            {{\n",
    "            \"main_characters\": [\"list of character names or roles\"],\n",
    "            \"character_descriptions\": [\"list of short character descriptions, same order as main_characters\"],\n",
    "            \"setting\": \"short description of where the story takes place\",\n",
    "            \"conflict_or_goal\": \"one-sentence summary of the story‚Äôs central conflict or goal\",\n",
    "            \"important_objects\": [\"list of important items, weapons, or artifacts\"],\n",
    "            \"mood_and_tone\": [\"one or more keywords: dramatic, mysterious, adventurous, romantic, comedic, emotional, dark\"],\n",
    "            \"key_sound_effects_and_emotions\": [\"list of notable sound effects (onomatopoeia) and strong emotions\"]\n",
    "            }}\n",
    "\n",
    "            Rules:\n",
    "            - Only output valid JSON, no explanations.\n",
    "            - Keep responses short and concise.\n",
    "            - Ensure the JSON matches the schema exactly.\n",
    "\n",
    "            Refined Manga Story:{refine_story}\n",
    "\n",
    "        \"\"\"\n",
    "    output=structured_model_MangaFeature.invoke(prompt)\n",
    "\n",
    "    return {\"extracted_features\":output.model_dump()}\n",
    "\n",
    "\n",
    "def character_makeup(state: MangaState):\n",
    "    refined_story = state.get(\"reviewed_story\") or state[\"refined_story\"]\n",
    "    extracted_feature = state['extracted_features']\n",
    "\n",
    "    # extracted_feature could be a dict or a JSON string depending on prior step\n",
    "    if isinstance(extracted_feature, dict):\n",
    "        extracted_feature_json = extracted_feature\n",
    "    elif isinstance(extracted_feature, str):\n",
    "        try:\n",
    "            extracted_feature_json = json.loads(extracted_feature)\n",
    "        except Exception:\n",
    "            extracted_feature_json = {\"main_characters\": [], \"character_descriptions\": []}\n",
    "    else:\n",
    "        extracted_feature_json = {}\n",
    "\n",
    "    prompt = f'''\n",
    "        You are a manga character designer. \n",
    "        Input: a short refined manga story and the extracted features (characters & brief descriptions).\n",
    "        Your job: produce a JSON array \"characters\" of detailed, stable character profiles suitable for repeated drawing across multiple panels.\n",
    "        You MUST output valid JSON ONLY and match the schema exactly.\n",
    "\n",
    "        Schema (for each character):\n",
    "        {{\n",
    "        \"name_or_role\": \"string\",\n",
    "        \"canonical_name\": \"string or null\",\n",
    "        \"age_range\": \"string\",\n",
    "        \"gender_presentation\": \"string or null\",\n",
    "        \"body_type\": \"string\",\n",
    "        \"height\": \"string or null\",\n",
    "        \"face\": \"short description (shape, nose, mouth, distinguishing facial features)\",\n",
    "        \"hair\": \"short description (color, style, length)\",\n",
    "        \"eyes\": \"short description (color, shape, special details like glow)\",\n",
    "        \"clothing\": \"short description (top, bottom, shoes, texture)\",\n",
    "        \"accessories\": [\"list of accessories\"],\n",
    "        \"color_palette\": [\"primary\", \"secondary\", \"accent\"],\n",
    "        \"notable_marks\": [\"scars, tattoos, birthmarks or empty list\"],\n",
    "        \"important_objects\": [\"items associated with this character\"],\n",
    "        \"signature_poses\": [\"list of 2-4 signature poses\"],\n",
    "        \"default_expressions\": [\"list of 3 typical expressions used in manga\"],\n",
    "        \"voice_short\": \"one-line descriptor of speaking voice or null\",\n",
    "        \"drawing_instructions\": \"manga-specific tips (line weight, shading, preferred camera angles)\",\n",
    "        \"visual_reference_prompt\": \"1-2 sentence prompt for an image generator to draw this character consistently\",\n",
    "        \"consistency_token\": \"unique_short_token (use this in downstream image prompts to ensure consistency)\"\n",
    "        }}\n",
    "\n",
    "        \n",
    "        Rules:\n",
    "        - You MUST create one character profile for every entry in \"main_characters\".\n",
    "        - The number of profiles in \"characters\" must exactly equal the number of \"main_characters\".\n",
    "        - Use the paired \"character_descriptions\" to enrich each profile.\n",
    "        - If details are missing, infer them from the refined story.\n",
    "        - Keep each profile short, clear, and usable for consistent drawing.\n",
    "        - Output only valid JSON in the format: {{ \"characters\": [ ... ] }}\n",
    "\n",
    "        Refined Story:\n",
    "        {refined_story}\n",
    "\n",
    "        Extracted Features:\n",
    "        {json.dumps(extracted_feature_json)}\n",
    "\n",
    "'''\n",
    "    output = structured_model_characterList.invoke(prompt)\n",
    "    return {\"character_feature\": output.model_dump()}\n",
    "\n",
    "\n",
    "def scene_feature_extractor(state: MangaState):\n",
    "    refined_story = state.get(\"reviewed_story\") or state[\"refined_story\"]\n",
    "    extracted_feature = state['extracted_features']\n",
    "    characters = state['character_feature']\n",
    "\n",
    "    # Normalize inputs\n",
    "    if isinstance(extracted_feature, dict):\n",
    "        features_json = extracted_feature\n",
    "    else:\n",
    "        try:\n",
    "            features_json = json.loads(extracted_feature)\n",
    "        except Exception:\n",
    "            features_json = {}\n",
    "\n",
    "    if isinstance(characters, dict):\n",
    "        characters_json = characters\n",
    "    else:\n",
    "        try:\n",
    "            characters_json = json.loads(characters)\n",
    "        except Exception:\n",
    "            characters_json = {}\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a Manga Scene Director.  \n",
    "    Input: a refined short manga story, extracted features, and character profiles.  \n",
    "    Task: break the story into **4‚Äì5 sequential scenes** (not panels yet).  \n",
    "    These scenes will later guide panel creation.  \n",
    "\n",
    "    Output Schema (JSON only):\n",
    "    {{\n",
    "      \"scenes\": [\n",
    "        {{\n",
    "          \"scene_number\": 1,\n",
    "          \"summary\": \"1‚Äì2 sentence summary of what happens in this scene\",\n",
    "          \"setting_details\": \"short description of location, mood, time\",\n",
    "          \"characters_involved\": [\"names_or_roles\"],\n",
    "          \"actions\": [\"list of short action phrases\"],\n",
    "          \"emotions\": [\"keywords for emotional tone\"],\n",
    "          \"potential_dialogues\": [\"list of possible dialogue lines (Name: text)\"],\n",
    "          \"inner_thoughts\": [\"list of possible inner thoughts with [brackets]\"],\n",
    "          \"sound_effects\": [\"list of onomatopoeia that could fit this scene\"],\n",
    "          \"visual_elements\": [\"list of key visual cues, colors, lighting, objects, symbols\"]\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "\n",
    "    Rules:\n",
    "    - Always output 4 or 5 scenes. Never fewer.  \n",
    "    - Each scene should feel like it could become one manga panel later.  \n",
    "    - Use only characters from the character profiles.  \n",
    "    - Keep dialogues short, natural, manga-style.  \n",
    "    - Be consistent with story tone and features.  \n",
    "    - Return **valid JSON only**.  \n",
    "\n",
    "    Refined Story:\n",
    "    {refined_story}\n",
    "\n",
    "    Extracted Features:\n",
    "    {json.dumps(features_json)}\n",
    "\n",
    "    Character Profiles:\n",
    "    {json.dumps(characters_json)}\n",
    "    \"\"\"\n",
    "\n",
    "    output = structured_model_scene.invoke(prompt)\n",
    "    return {\"scene_features\": output.model_dump()}\n",
    "\n",
    "\n",
    "def manga_director(state: MangaState):\n",
    "    refined_story = state.get(\"reviewed_story\") or state[\"refined_story\"]\n",
    "    features = state['extracted_features']\n",
    "    characters = state['character_feature']\n",
    "    scenes = state['scene_features']   # ‚úÖ add this\n",
    "    #scenes = state.get('scene_features') or state.get('scene_feature') or {}\n",
    "    prompt = f\"\"\"\n",
    "    You are a Manga Director. \n",
    "    Your job is to take the refined story, extracted features, character profiles, \n",
    "    and pre-extracted scene features, and create a ONE-PAGE manga script divided into **exactly 4‚Äì5 panels**.\n",
    "\n",
    "    Schema:\n",
    "    {{\n",
    "      \"page_number\": 1,\n",
    "      \"panels\": [\n",
    "        {{\n",
    "          \"panel_number\": 1,\n",
    "          \"scene_description\": \"string (describe scene, setting, mood, camera angle)\",\n",
    "          \"characters_present\": [\"list of character names_or_roles\"],\n",
    "          \"actions\": [\"short action phrases\"],\n",
    "          \"dialogues\": [\"Name: text\"],\n",
    "          \"inner_thoughts\": [\"list of inner thoughts if any\"],\n",
    "          \"sound_effects\": [\"list of onomatopoeia like BAM, WHOOSH\"],\n",
    "          \"visual_elements\": [\"list of key visual cues, colors, lighting, objects, symbols\"]\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "\n",
    "    Rules:\n",
    "    - Output **exactly 4 or 5 panels**. Never fewer, never more.\n",
    "    - Each panel should map to one of the extracted \"scenes\" (use `scene_reference` field).\n",
    "    - Use only characters from the given profiles.\n",
    "    - Keep dialogues short, natural, manga-style.\n",
    "    - Balance between action, emotion, and pacing.\n",
    "    - Ensure JSON output only, no extra explanation.\n",
    "\n",
    "    Refined Story: {refined_story}\n",
    "\n",
    "    Extracted Features: {features}\n",
    "\n",
    "    Character Profiles: {characters}\n",
    "\n",
    "    Scene Features: {scenes}\n",
    "    \"\"\"\n",
    "\n",
    "    output = structured_model_director.invoke(prompt)\n",
    "\n",
    "    # Enforce 4‚Äì5 panels\n",
    "    if \"panels\" in output and (len(output[\"panels\"]) < 4 or len(output[\"panels\"]) > 5):\n",
    "        retry_prompt = prompt + \"\\n\\n‚ö†Ô∏è Reminder: You must output 4‚Äì5 panels, not fewer, not more.\"\n",
    "        output = structured_model_director.invoke(retry_prompt)\n",
    "\n",
    "    return {\"panel_scenes\": output.model_dump()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def manga_comic_generator(state: MangaState):\n",
    "#     refined_story = state[\"refined_story\"]\n",
    "#     features = state[\"extracted_feature\"]\n",
    "#     characters = state[\"character_feature\"]\n",
    "#     scenes = state[\"scene_features\"]\n",
    "#     panels = state[\"panel_scenes\"]\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "#     You are an expert Manga Illustrator AI.\n",
    "#     Your task is to generate **manga-style comic panels** based on the story, features, character designs, scene features, and director‚Äôs panel instructions.\n",
    "\n",
    "#     # Global Rules for All Panels:\n",
    "#     - Style: black-and-white manga, clean line art, screentone shading, dramatic lighting.\n",
    "#     - Characters: must remain visually consistent using their \"consistency_token\" + \"visual_reference_prompt\".\n",
    "#     - Each panel must be fully self-contained: repeat full character and background descriptions in every panel prompt, because images are generated independently.\n",
    "#     - Backgrounds: keep recurring settings identical across panels by always describing them in the same way.\n",
    "#     - Props/Clothing: must remain identical across all panels (e.g., same uniform, hairstyle, or objects) unless explicitly changed in the director‚Äôs script.\n",
    "#     - Emotions: exaggerate expressions (wide eyes, sweat drops, speed lines, dramatic shadows).\n",
    "#     - Sound Effects: always add manga-style onomatopoeia text when listed.\n",
    "#     - Dialogues & Inner Thoughts: display exactly as provided in the script.\n",
    "#     - Camera framing: always specify (e.g., \"close-up\", \"wide shot\", \"medium shot\").\n",
    "#     - Always end each panel prompt with: \"in consistent manga style\".\n",
    "\n",
    "#     # Inputs:\n",
    "#     Refined Story:\n",
    "#     {refined_story}\n",
    "\n",
    "#     Extracted Features:\n",
    "#     {json.dumps(features, indent=2)}\n",
    "\n",
    "#     Character Profiles (use consistency_token and visual_reference_prompt for each character):\n",
    "#     {json.dumps(characters, indent=2)}\n",
    "\n",
    "#     Scene Features:\n",
    "#     {json.dumps(scenes, indent=2)}\n",
    "\n",
    "#     Director‚Äôs Panel Script:\n",
    "#     {json.dumps(panels, indent=2)}\n",
    "\n",
    "#     # Output Instruction:\n",
    "#     For each panel in the Director‚Äôs Panel Script, generate an **image prompt** formatted as:\n",
    "\n",
    "#     {{\n",
    "#     \"panel_number\": <int>,\n",
    "#     \"image_prompt\": \"<1‚Äì3 sentences describing exactly what to draw: include characters with their 'consistency_token' and full 'visual_reference_prompt', poses, expressions, actions, identical setting description, camera angle, sound effects, and dialogues/inner thoughts. End with 'in consistent manga style'.>\"\n",
    "#     }}\n",
    "\n",
    "#     Ensure:\n",
    "#     - Each panel‚Äôs \"image_prompt\" is fully self-contained (do not use 'as previously described').\n",
    "#     - Characters must always include their \"consistency_token\" and \"visual_reference_prompt\".\n",
    "#     - Backgrounds and clothing must always be described consistently in the same wording.\n",
    "#     - Include all dialogues and inner thoughts visually in the panel description.\n",
    "#     - Use sound effects from the panel if listed.\n",
    "#     - Only output valid JSON: a list of panel prompts.\n",
    "#     \"\"\"\n",
    "\n",
    "#     return {\"manga_image_prompts\": structured_model_Mangaprompt.invoke(prompt)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def manga_comic_generator(state: MangaState):\n",
    "    refined_story = state.get(\"reviewed_story\") or state[\"refined_story\"]\n",
    "    features = state[\"extracted_features\"]\n",
    "    characters = state[\"character_feature\"]\n",
    "    scenes = state[\"scene_features\"]\n",
    "    panels = state[\"panel_scenes\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a Manga Illustrator AI.\n",
    "    Generate compact **manga-style comic panel prompts** (‚â§70 tokens each).\n",
    "\n",
    "    # Rules:\n",
    "    - Style: black-and-white manga, clean line art, screentone shading.\n",
    "    - Characters: always use their \"consistency_token\" + short visual keywords.\n",
    "    - Backgrounds: keep consistent, describe in ‚â§5 words.\n",
    "    - Panel prompt must be **1 short sentence**, keyword-style.\n",
    "    - Always include: camera angle, characters, action, scene, emotions, sound, dialogue.\n",
    "    - End each with \"in consistent manga style\".\n",
    "\n",
    "    # Inputs:\n",
    "    Refined Story:\n",
    "    {refined_story}\n",
    "\n",
    "    Extracted Features:\n",
    "    {json.dumps(features, indent=2)}\n",
    "\n",
    "    Character Profiles (use consistency_token and visual_reference_prompt for each character):\n",
    "    {json.dumps(characters, indent=2)}\n",
    "\n",
    "    Scene Features:\n",
    "    {json.dumps(scenes, indent=2)}\n",
    "\n",
    "    Director‚Äôs Panel Script:\n",
    "    {json.dumps(panels, indent=2)}\n",
    "\n",
    "    # Output Instruction:\n",
    "    For each panel in the Director‚Äôs Panel Script, generate an **image prompt** formatted as:\n",
    "\n",
    "    {{\n",
    "    \"panel_number\": <int>,\n",
    "    \"image_prompt\": \"<1‚Äì3 sentences describing exactly what to draw: include characters with their 'consistency_token' and full 'visual_reference_prompt', poses, expressions, actions, identical setting description, camera angle, sound effects, and dialogues/inner thoughts. End with 'in consistent manga style'.>\"\n",
    "    }}\n",
    "\n",
    "    Ensure:\n",
    "    - Each panel‚Äôs \"image_prompt\" is fully self-contained (do not use 'as previously described').\n",
    "    - Characters must always include their \"consistency_token\" and \"visual_reference_prompt\".\n",
    "    - Backgrounds and clothing must always be described consistently in the same wording.\n",
    "    - Include all dialogues and inner thoughts visually in the panel description.\n",
    "    - Use sound effects from the panel if listed.\n",
    "    - Only output valid JSON: a list of panel prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    return {\"manga_image_prompts\": structured_model_Mangaprompt.invoke(prompt).model_dump()}\n",
    "\n",
    "def prompt_analyzer(state: MangaState):\n",
    "    \"\"\"\n",
    "    Analyzes ALL manga image prompts collectively for overall page quality using Pydantic schemas.\n",
    "    Returns structured analysis and decides whether to regenerate or proceed to END.\n",
    "    \"\"\"\n",
    "    prompts_data = state.get(\"manga_image_prompts\", {})\n",
    "    panel_prompts = prompts_data.get(\"panel_prompts\", [])\n",
    "    attempts = state.get(\"generation_attempts\", 0)\n",
    "    max_attempts = state.get(\"max_attempts\", 5)  # Get from state, default to 5\n",
    "    \n",
    "    if not panel_prompts:\n",
    "        # Return minimal analysis for empty prompts using Pydantic\n",
    "        empty_analysis = PromptAnalysisResult(\n",
    "            panel_analyses=[],\n",
    "            overall_score=0.0,\n",
    "            total_panels=0,\n",
    "            panels_needing_improvement=0,\n",
    "            needs_regeneration=False,\n",
    "            generation_attempt=attempts + 1,\n",
    "            max_attempts_reached=attempts >= max_attempts\n",
    "        )\n",
    "        return {\n",
    "            \"prompt_analysis\": empty_analysis.model_dump(),\n",
    "            \"generation_attempts\": attempts + 1\n",
    "        }\n",
    "    \n",
    "    # Prepare all prompts for collective analysis\n",
    "    all_prompts_text = \"\\n\\n\".join([\n",
    "        f\"Panel {panel.get('panel_number', i+1)}: {panel.get('image_prompt', '')}\"\n",
    "        for i, panel in enumerate(panel_prompts)\n",
    "    ])\n",
    "    \n",
    "    # Single comprehensive analysis prompt for the entire manga page\n",
    "    collective_analysis_prompt = f\"\"\"\n",
    "    You are an expert manga artist and prompt engineer for AI image generation.\n",
    "    Analyze this COMPLETE MANGA PAGE (all panels together) according to professional manga standards:\n",
    "\n",
    "    MANGA PAGE QUALITY CRITERIA:\n",
    "    1. Character Consistency: Same consistency_tokens and visual descriptions across ALL panels\n",
    "    2. Visual Continuity: Consistent backgrounds, clothing, props throughout the page\n",
    "    3. Manga Style Elements: All panels mention screentone, line art, dramatic expressions\n",
    "    4. Story Flow: Logical narrative progression from panel 1 to final panel\n",
    "    5. Technical Quality: All panels end with \"in consistent manga style\"\n",
    "    6. Emotional Arc: Clear emotional journey across the entire page\n",
    "    7. Panel Cohesion: Panels work together as a unified comic page\n",
    "\n",
    "    COMPLETE MANGA PAGE TO ANALYZE:\n",
    "    {all_prompts_text}\n",
    "\n",
    "    Analyze the ENTIRE PAGE and provide:\n",
    "    - Individual panel analysis for each panel\n",
    "    - Overall page quality assessment\n",
    "    - Focus on consistency and flow between panels\n",
    "    - Overall score should reflect page cohesion, not just individual panel quality\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Use your existing structured model for comprehensive analysis\n",
    "        full_analysis = structured_model_PromptAnalysis.invoke(collective_analysis_prompt)\n",
    "        analysis_result = full_analysis.model_dump()\n",
    "        \n",
    "        # Ensure we have analysis for all panels\n",
    "        panel_analyses = analysis_result.get(\"panel_analyses\", [])\n",
    "        if len(panel_analyses) != len(panel_prompts):\n",
    "            # Fill missing panel analyses\n",
    "            for i, panel in enumerate(panel_prompts):\n",
    "                if i >= len(panel_analyses):\n",
    "                    panel_num = panel.get(\"panel_number\", i+1)\n",
    "                    # Create analysis based on overall assessment\n",
    "                    panel_analyses.append({\n",
    "                        \"panel_number\": panel_num,\n",
    "                        \"quality_score\": max(1, int(analysis_result.get(\"overall_score\", 5))),\n",
    "                        \"issues\": [\"Needs consistency review\"],\n",
    "                        \"strengths\": [\"Part of manga page\"],\n",
    "                        \"needs_improvement\": analysis_result.get(\"overall_score\", 5) < 7.5,\n",
    "                        \"suggested_fixes\": [\"Improve consistency with other panels\"]\n",
    "                    })\n",
    "        \n",
    "        analysis_result[\"panel_analyses\"] = panel_analyses\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Fallback: create comprehensive analysis for all panels\n",
    "        fallback_analyses = []\n",
    "        for i, panel in enumerate(panel_prompts):\n",
    "            panel_num = panel.get(\"panel_number\", i+1)\n",
    "            fallback_analyses.append({\n",
    "                \"panel_number\": panel_num,\n",
    "                \"quality_score\": 5,\n",
    "                \"issues\": [f\"Collective analysis failed: {str(e)[:50]}\"],\n",
    "                \"strengths\": [\"Contains manga elements\"],\n",
    "                \"needs_improvement\": True,\n",
    "                \"suggested_fixes\": [\"Regenerate with better page consistency\"]\n",
    "            })\n",
    "        \n",
    "        analysis_result = {\n",
    "            \"panel_analyses\": fallback_analyses,\n",
    "            \"overall_score\": 5.0,\n",
    "            \"total_panels\": len(panel_prompts),\n",
    "            \"panels_needing_improvement\": len(panel_prompts),\n",
    "            \"needs_regeneration\": attempts < max_attempts,\n",
    "            \"generation_attempt\": attempts + 1,\n",
    "            \"max_attempts_reached\": attempts >= max_attempts\n",
    "        }\n",
    "    \n",
    "    # Enhanced decision logic for overall page quality\n",
    "    overall_score = analysis_result.get(\"overall_score\", 0)\n",
    "    panels_needing_improvement = analysis_result.get(\"panels_needing_improvement\", len(panel_prompts))\n",
    "    \n",
    "    # More sophisticated regeneration criteria\n",
    "    page_quality_threshold = 7.5  # Higher threshold for overall page quality\n",
    "    max_problematic_panels = 1    # Allow at most 1 panel to need improvement\n",
    "    \n",
    "    should_regenerate = (\n",
    "        (overall_score < page_quality_threshold or panels_needing_improvement > max_problematic_panels)\n",
    "        and attempts < max_attempts\n",
    "    )\n",
    "    \n",
    "    # Update final analysis\n",
    "    analysis_result[\"needs_regeneration\"] = should_regenerate\n",
    "    analysis_result[\"generation_attempt\"] = attempts + 1\n",
    "    analysis_result[\"max_attempts_reached\"] = attempts >= max_attempts\n",
    "    \n",
    "    return {\n",
    "        \"prompt_analysis\": analysis_result,\n",
    "        \"generation_attempts\": attempts + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4bdb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rules for Drawing:\n",
    "# - Style: black-and-white manga style, with clean line art, screentone shading, and dramatic lighting.\n",
    "# - Characters: must remain visually consistent across all panels using their \"consistency_token\" and \"visual_reference_prompt\".\n",
    "# - Composition: follow the panel description (camera angle, action, emotion).\n",
    "# - Emotions: exaggerate expressions (wide eyes, sweat drops, speed lines, dramatic shadows).\n",
    "# - Sound Effects: integrate onomatopoeia text (e.g., \"BAM!\", \"WHOOSH!\") in stylized manga lettering.\n",
    "# - Dialogues & Inner Thoughts: include all dialogues and inner thoughts from the director‚Äôs panel instructions.\n",
    "# - Do not invent new characters or objects outside what‚Äôs provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc689edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# story=prompt_refinner({'input_story':\"A boy name Ibad fall in love with a girl named Aisha.\"})\n",
    "# features=feature_extractor({\"refined_story\":story})\n",
    "# character_mkp=character_makeup({\"refined_story\": story[\"refined_story\"], \"extracted_features\": features[\"extracted_feature\"]})\n",
    "# scenes_char=scene_feature_extractor({\"refined_story\": story[\"refined_story\"], \"extracted_feature\": features[\"extracted_feature\"],\"character_feature\": character_mkp[\"character_feature\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d43c4801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# story='''\n",
    "# Ibad saw Aisha across the crowded schoolyard, and his heart *POW!* skipped a '\n",
    "#  \"beat. [She's...an angel!] He clumsily rushed towards her, tripping over his \"\n",
    "#  'own feet ‚Äì *THUD!* Aisha giggled, her laughter like wind chimes. \"H-hello!\" '\n",
    "#  'Ibad stammered, face burning crimson. Aisha smiled, and Ibad knew, with '\n",
    "#  'absolute certainty, that his life had irrevocably changed. GYAa!')\n",
    "# '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5dc762b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional routing function\n",
    "def should_regenerate_prompts(state: MangaState):\n",
    "    \"\"\"Route based on overall page quality analysis using Pydantic structured results\"\"\"\n",
    "    analysis = state.get(\"prompt_analysis\", {})\n",
    "    needs_regen = analysis.get(\"needs_regeneration\", False)\n",
    "    overall_score = analysis.get(\"overall_score\", 0)\n",
    "    max_attempts_reached = analysis.get(\"max_attempts_reached\", False)\n",
    "    \n",
    "    # Enhanced routing logic\n",
    "    if max_attempts_reached:\n",
    "        return END  # Stop after max attempts\n",
    "    elif needs_regen and overall_score < 9.5:\n",
    "        return \"manga_comic_generator\"  # Loop back for page-level improvement  \n",
    "    else:\n",
    "        return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f117aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating Graph\n",
    "\n",
    "graph=StateGraph(MangaState)\n",
    "\n",
    "## creating Nodes\n",
    "\n",
    "graph.add_node(\"prompt_refinner\",prompt_refinner)\n",
    "graph.add_node(\"review\", review_node)\n",
    "graph.add_node(\"feature_extractor\",feature_extractor)\n",
    "# graph.add_node(\"character_makeup\",character_makeup)\n",
    "# graph.add_node(\"scene_feature_extractor\", scene_feature_extractor) \n",
    "# graph.add_node(\"manga_director\",manga_director)\n",
    "# graph.add_node(\"manga_comic_generator\",manga_comic_generator)\n",
    "# graph.add_node(\"prompt_analyzer\", prompt_analyzer)\n",
    "\n",
    "\n",
    "#Creating Edges\n",
    "\n",
    "graph.add_edge(START,\"prompt_refinner\")\n",
    "graph.add_edge(\"prompt_refinner\", \"review\")\n",
    "graph.add_edge(\"review\", \"feature_extractor\")\n",
    "graph.add_edge(\"feature_extractor\", END)\n",
    "\n",
    "# graph.add_edge(\"review\", \"feature_extractor\")\n",
    "# graph.add_edge(\"feature_extractor\",\"character_makeup\")\n",
    "# graph.add_edge(\"character_makeup\",\"scene_feature_extractor\")\n",
    "# graph.add_edge(\"scene_feature_extractor\",\"manga_director\")\n",
    "# graph.add_edge(\"manga_director\",\"manga_comic_generator\")\n",
    "# #graph.add_edge(\"manga_comic_generator\", \"prompt_analyzer\")\n",
    "# graph.add_edge(\"manga_comic_generator\", END)\n",
    "\n",
    "# # Add conditional edge for the quality control loop\n",
    "# graph.add_conditional_edges(\n",
    "#     \"prompt_analyzer\",\n",
    "#     should_regenerate_prompts,\n",
    "#     {\n",
    "#         \"manga_comic_generator\": \"manga_comic_generator\",  # Loop back\n",
    "#         END: END  # Exit condition\n",
    "#     }\n",
    "# )\n",
    "\n",
    "checkpointer=InMemorySaver()\n",
    "\n",
    "workflow=graph.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "539afe06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(workflow.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a970f255",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (1633627912.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[40], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    prompt_example=\"\"\"\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "prompt_example=\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a8996b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp\n",
      "Please retry in 10.103917562s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 10\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp\n",
      "Please retry in 7.989004466s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 7\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp\n",
      "Please retry in 3.889575832s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 3\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp\n",
      "Please retry in 55.782445993s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanga-001\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[0;32m      3\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_story\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA boy name Ibad fall in love with a girl named Aisha.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneration_attempts\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_attempts\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m  \n\u001b[0;32m      7\u001b[0m }\n\u001b[1;32m---> 10\u001b[0m initial \u001b[38;5;241m=\u001b[39m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(initial[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__interrupt__\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# -> [Interrupt(value={'instruction': ..., 'content': ...})]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langgraph\\pregel\\main.py:3026\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[0;32m   3023\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3024\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 3026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   3027\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3028\u001b[0m     config,\n\u001b[0;32m   3029\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m   3030\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3032\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[0;32m   3033\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[0;32m   3034\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   3035\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   3036\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   3037\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[0;32m   3038\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3039\u001b[0m ):\n\u001b[0;32m   3040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3041\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langgraph\\pregel\\main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[0;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[0;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[0;32m   2652\u001b[0m ):\n\u001b[0;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[0;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[0;32m   2656\u001b[0m     )\n\u001b[0;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langgraph\\pregel\\_runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[0;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langgraph\\_internal\\_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langgraph\\_internal\\_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[33], line 22\u001b[0m, in \u001b[0;36mprompt_refinner\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      2\u001b[0m user_story\u001b[38;5;241m=\u001b[39mstate[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_story\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m        You are a professional manga storyteller. \u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m        Your job is to take a short user query and refine it into a concise manga-style story \u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124m        Refined Manga Story:\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m---> 22\u001b[0m refine_output\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(refine_output)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_google_genai\\chat_models.py:1490\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI.invoke\u001b[1;34m(self, input, config, code_execution, stop, **kwargs)\u001b[0m\n\u001b[0;32m   1485\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1486\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1487\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTools are already defined.code_execution tool can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be defined\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1488\u001b[0m         )\n\u001b[1;32m-> 1490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:393\u001b[0m, in \u001b[0;36minvoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    396\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    397\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    398\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    399\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    400\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    401\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    402\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    403\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    404\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    405\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1019\u001b[0m, in \u001b[0;36mgenerate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         output\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1012\u001b[0m             RunInfo(run_id\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mrun_id) \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers\n\u001b[0;32m   1013\u001b[0m         ]\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m-> 1019\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   1020\u001b[0m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1021\u001b[0m     callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1023\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m   1024\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:837\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    826\u001b[0m run_managers \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chat_model_start(\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m    828\u001b[0m     messages_to_trace,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    833\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(messages),\n\u001b[0;32m    834\u001b[0m )\n\u001b[0;32m    835\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    836\u001b[0m input_messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 837\u001b[0m     _normalize_messages(message_list) \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m    838\u001b[0m ]\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1085\u001b[0m, in \u001b[0;36m_generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1084\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_LC_ID_PREFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1085\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_new_token(\n\u001b[0;32m   1086\u001b[0m             cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m, chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent), chunk\u001b[38;5;241m=\u001b[39mchunk\n\u001b[0;32m   1087\u001b[0m         )\n\u001b[0;32m   1088\u001b[0m     chunks\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[0;32m   1089\u001b[0m result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_google_genai\\chat_models.py:1597\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m   1571\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1572\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1583\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1584\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m   1585\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m   1586\u001b[0m         messages,\n\u001b[0;32m   1587\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1595\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1596\u001b[0m     )\n\u001b[1;32m-> 1597\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m _chat_with_retry(\n\u001b[0;32m   1598\u001b[0m         request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m   1599\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1600\u001b[0m         generation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mgenerate_content,\n\u001b[0;32m   1601\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata,\n\u001b[0;32m   1602\u001b[0m     )\n\u001b[0;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_google_genai\\chat_models.py:247\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    240\u001b[0m params \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    241\u001b[0m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (request \u001b[38;5;241m:=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[0;32m    246\u001b[0m )\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chat_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\tenacity\\__init__.py:338\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    336\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    337\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\tenacity\\__init__.py:487\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[0;32m    486\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[1;32m--> 487\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\tenacity\\nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(seconds)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"manga-001\"}}\n",
    "\n",
    "initial_state = {\n",
    "    'input_story': \"A boy name Ibad fall in love with a girl named Aisha.\",\n",
    "    'generation_attempts': 0,\n",
    "    'max_attempts': 2  \n",
    "}\n",
    "\n",
    "\n",
    "initial = workflow.invoke(initial_state,config=config)\n",
    "\n",
    "print(initial[\"__interrupt__\"])  # -> [Interrupt(value={'instruction': ..., 'content': ...})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca0bb0c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 Function calling is not enabled for models/gemma-3-1b-it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_google_genai\\chat_models.py:216\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mFailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[0m, in \u001b[0;36mgenerate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    833\u001b[0m flattened_params \u001b[38;5;241m=\u001b[39m [model, contents]\n\u001b[0;32m    834\u001b[0m has_flattened_params \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 835\u001b[0m     \u001b[38;5;28mlen\u001b[39m([param \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m flattened_params \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m has_flattened_params:\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    210\u001b[0m         error_list,\n\u001b[0;32m    211\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    212\u001b[0m         original_timeout,\n\u001b[0;32m    213\u001b[0m     )\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgument\u001b[0m: 400 Function calling is not enabled for models/gemma-3-1b-it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mChatGoogleGenerativeAIError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Resume with the edited text from the reviewer\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m final_after_review \u001b[38;5;241m=\u001b[39m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIbad azam drifted through the noisy school hall, lost in thought, when a sudden, pure laugh pierced the din. *WHOOSH!* There she was‚ÄîAisha, gently helping a tiny kouhai pick up their scattered pencils, her smile a beacon of pure kindness. *THUMP-THUMP! THUMP-THUMP!* Ibad‚Äôs heart exploded in his chest, his face instantly erupting in a fiery blush. [Is this... love at first sight?!] He stood frozen, a dazed look in his wide eyes as Aisha briefly met his gaze with a soft, knowing nod. *Gyaa!* His world had just tilted on its axis.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_after_review[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviewed_story\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# -> \"Improved draft after review\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langgraph\\pregel\\main.py:3026\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[0;32m   3023\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3024\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 3026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   3027\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3028\u001b[0m     config,\n\u001b[0;32m   3029\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m   3030\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3032\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[0;32m   3033\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[0;32m   3034\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   3035\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   3036\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   3037\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[0;32m   3038\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3039\u001b[0m ):\n\u001b[0;32m   3040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3041\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langgraph\\pregel\\main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[0;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[0;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[0;32m   2652\u001b[0m ):\n\u001b[0;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[0;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[0;32m   2656\u001b[0m     )\n\u001b[0;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langgraph\\pregel\\_runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[0;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langgraph\\_internal\\_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langgraph\\_internal\\_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[20], line 110\u001b[0m, in \u001b[0;36mfeature_extractor\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     83\u001b[0m refine_story \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviewed_story\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefined_story\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     85\u001b[0m prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     86\u001b[0m \n\u001b[0;32m     87\u001b[0m \u001b[38;5;124m        You are a manga story analyzer. \u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m--> 110\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[43mstructured_model_MangaFeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextracted_features\u001b[39m\u001b[38;5;124m\"\u001b[39m:output\u001b[38;5;241m.\u001b[39mmodel_dump()}\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_core\\runnables\\base.py:3080\u001b[0m, in \u001b[0;36minvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3071\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n\u001b[0;32m   3072\u001b[0m     CONTEXT_CONFIG_PREFIX,\n\u001b[0;32m   3073\u001b[0m     _key_from_id,\n\u001b[0;32m   3074\u001b[0m )\n\u001b[0;32m   3076\u001b[0m \u001b[38;5;66;03m# get all specs\u001b[39;00m\n\u001b[0;32m   3077\u001b[0m all_specs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3078\u001b[0m     (spec, idx)\n\u001b[0;32m   3079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps)\n\u001b[1;32m-> 3080\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m spec \u001b[38;5;129;01min\u001b[39;00m step\u001b[38;5;241m.\u001b[39mconfig_specs\n\u001b[0;32m   3081\u001b[0m ]\n\u001b[0;32m   3082\u001b[0m \u001b[38;5;66;03m# calculate context dependencies\u001b[39;00m\n\u001b[0;32m   3083\u001b[0m specs_by_pos \u001b[38;5;241m=\u001b[39m groupby(\n\u001b[0;32m   3084\u001b[0m     [tup \u001b[38;5;28;01mfor\u001b[39;00m tup \u001b[38;5;129;01min\u001b[39;00m all_specs \u001b[38;5;28;01mif\u001b[39;00m tup[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mstartswith(CONTEXT_CONFIG_PREFIX)],\n\u001b[0;32m   3085\u001b[0m     itemgetter(\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m   3086\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_core\\runnables\\base.py:5495\u001b[0m, in \u001b[0;36minvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5469\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   5470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_listeners\u001b[39m(\n\u001b[0;32m   5471\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5481\u001b[0m     ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5482\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RunnableEach[Input, Output]:\n\u001b[0;32m   5483\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Bind lifecycle listeners to a ``Runnable``, returning a new ``Runnable``.\u001b[39;00m\n\u001b[0;32m   5484\u001b[0m \n\u001b[0;32m   5485\u001b[0m \u001b[38;5;124;03m    The ``Run`` object contains information about the run, including its ``id``,\u001b[39;00m\n\u001b[0;32m   5486\u001b[0m \u001b[38;5;124;03m    ``type``, ``input``, ``output``, ``error``, ``start_time``, ``end_time``, and\u001b[39;00m\n\u001b[0;32m   5487\u001b[0m \u001b[38;5;124;03m    any tags or metadata added to the run.\u001b[39;00m\n\u001b[0;32m   5488\u001b[0m \n\u001b[0;32m   5489\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   5490\u001b[0m \u001b[38;5;124;03m        on_start: Called before the ``Runnable`` starts running, with the ``Run``\u001b[39;00m\n\u001b[0;32m   5491\u001b[0m \u001b[38;5;124;03m            object. Defaults to None.\u001b[39;00m\n\u001b[0;32m   5492\u001b[0m \u001b[38;5;124;03m        on_end: Called after the ``Runnable`` finishes running, with the ``Run``\u001b[39;00m\n\u001b[0;32m   5493\u001b[0m \u001b[38;5;124;03m            object. Defaults to None.\u001b[39;00m\n\u001b[0;32m   5494\u001b[0m \u001b[38;5;124;03m        on_error: Called if the ``Runnable`` throws an error, with the ``Run``\u001b[39;00m\n\u001b[1;32m-> 5495\u001b[0m \u001b[38;5;124;03m            object. Defaults to None.\u001b[39;00m\n\u001b[0;32m   5496\u001b[0m \n\u001b[0;32m   5497\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m   5498\u001b[0m \u001b[38;5;124;03m        A new ``Runnable`` with the listeners bound.\u001b[39;00m\n\u001b[0;32m   5499\u001b[0m \n\u001b[0;32m   5500\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   5501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableEach(\n\u001b[0;32m   5502\u001b[0m         bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mwith_listeners(\n\u001b[0;32m   5503\u001b[0m             on_start\u001b[38;5;241m=\u001b[39mon_start, on_end\u001b[38;5;241m=\u001b[39mon_end, on_error\u001b[38;5;241m=\u001b[39mon_error\n\u001b[0;32m   5504\u001b[0m         )\n\u001b[0;32m   5505\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_google_genai\\chat_models.py:1490\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI.invoke\u001b[1;34m(self, input, config, code_execution, stop, **kwargs)\u001b[0m\n\u001b[0;32m   1485\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1486\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1487\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTools are already defined.code_execution tool can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be defined\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1488\u001b[0m         )\n\u001b[1;32m-> 1490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:393\u001b[0m, in \u001b[0;36minvoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    396\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    397\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    398\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    399\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    400\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    401\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    402\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    403\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    404\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    405\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1019\u001b[0m, in \u001b[0;36mgenerate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         output\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1012\u001b[0m             RunInfo(run_id\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mrun_id) \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers\n\u001b[0;32m   1013\u001b[0m         ]\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m-> 1019\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   1020\u001b[0m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1021\u001b[0m     callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1023\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m   1024\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:837\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    826\u001b[0m run_managers \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chat_model_start(\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m    828\u001b[0m     messages_to_trace,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    833\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(messages),\n\u001b[0;32m    834\u001b[0m )\n\u001b[0;32m    835\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    836\u001b[0m input_messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 837\u001b[0m     _normalize_messages(message_list) \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m    838\u001b[0m ]\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1085\u001b[0m, in \u001b[0;36m_generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1084\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_LC_ID_PREFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1085\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_new_token(\n\u001b[0;32m   1086\u001b[0m             cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m, chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent), chunk\u001b[38;5;241m=\u001b[39mchunk\n\u001b[0;32m   1087\u001b[0m         )\n\u001b[0;32m   1088\u001b[0m     chunks\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[0;32m   1089\u001b[0m result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_google_genai\\chat_models.py:1597\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m   1571\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1572\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1583\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1584\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m   1585\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m   1586\u001b[0m         messages,\n\u001b[0;32m   1587\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1595\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1596\u001b[0m     )\n\u001b[1;32m-> 1597\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m _chat_with_retry(\n\u001b[0;32m   1598\u001b[0m         request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m   1599\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1600\u001b[0m         generation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mgenerate_content,\n\u001b[0;32m   1601\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata,\n\u001b[0;32m   1602\u001b[0m     )\n\u001b[0;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_google_genai\\chat_models.py:247\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    240\u001b[0m params \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    241\u001b[0m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (request \u001b[38;5;241m:=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[0;32m    246\u001b[0m )\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chat_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\tenacity\\__init__.py:338\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    336\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    337\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\tenacity\\__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 477\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\tenacity\\__init__.py:378\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    376\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\tenacity\\__init__.py:400\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\concurrent\\futures\\_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\tenacity\\__init__.py:480\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    482\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Haider\\anaconda3\\envs\\global\\lib\\site-packages\\langchain_google_genai\\chat_models.py:227\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mResourceExhausted \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;66;03m# Handle quota-exceeded error with recommended retry delay\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry_after\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mretry_after \u001b[38;5;241m<\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwait_exponential_max\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m60.0\u001b[39m\n\u001b[0;32m    234\u001b[0m     ):\n",
      "\u001b[1;31mChatGoogleGenerativeAIError\u001b[0m: Invalid argument provided to Gemini: 400 Function calling is not enabled for models/gemma-3-1b-it"
     ]
    }
   ],
   "source": [
    "# Resume with the edited text from the reviewer\n",
    "\n",
    "\n",
    "final_after_review = workflow.invoke(\n",
    "    Command(resume=\"Ibad azam drifted through the noisy school hall, lost in thought, when a sudden, pure laugh pierced the din. *WHOOSH!* There she was‚ÄîAisha, gently helping a tiny kouhai pick up their scattered pencils, her smile a beacon of pure kindness. *THUMP-THUMP! THUMP-THUMP!* Ibad‚Äôs heart exploded in his chest, his face instantly erupting in a fiery blush. [Is this... love at first sight?!] He stood frozen, a dazed look in his wide eyes as Aisha briefly met his gaze with a soft, knowing nod. *Gyaa!* His world had just tilted on its axis.\"),\n",
    "    config=config\n",
    ")\n",
    "print(final_after_review[\"reviewed_story\"])  # -> \"Improved draft after review\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a02b1dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'main_characters': ['Ibad azam', 'Aisha'], 'character_descriptions': ['A shy student who experiences love at first sight and is easily flustered.', 'A kind and gentle student with a pure laugh and a beacon of kindness.'], 'setting': 'A noisy school hall', 'conflict_or_goal': 'Ibad experiences overwhelming love at first sight for Aisha, tilting his world on its axis.', 'important_objects': [], 'mood_and_tone': ['romantic', 'emotional'], 'key_sound_effects_and_emotions': ['WHOOSH!', 'THUMP-THUMP!', 'Gyaa!', 'love at first sight', 'fiery blush', 'dazed', 'overwhelmed']}\n"
     ]
    }
   ],
   "source": [
    "print(final_after_review[\"extracted_features\"])  # -> Extracted features as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923629db",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state[\"prompt_analysis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830fe435",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state[\"input_story\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee28b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state[\"refined_story\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state[\"extracted_features\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state[\"character_feature\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951aace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state[\"scene_features\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ff4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state[\"panel_scenes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(final_state[\"manga_image_prompts\"][\"panel_prompts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20b01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_number': 1,\n",
       " 'panel_prompts': [{'panel_number': 1,\n",
       "   'image_prompt': \"Medium shot: AISHA_MNG (radiant, laughing) in sun-drenched courtyard, head tilted back, smiling; IBAD_MNG (flustered, wide-eyed) freezes mid-stride, bento box CLATTERING, inner thought: [Her smile... it's like a supernova!]. in consistent manga style.\"},\n",
       "  {'panel_number': 2,\n",
       "   'image_prompt': 'Close-up: IBAD_MNG (flustered, blushing) face, cheeks volcanic red, steam rising, heart DOKI-DOKI wildly in sun-drenched courtyard as AISHA_MNG (gentle smile, sparkling eyes) turns, inner thought: [Her eyes sparkling like distant galaxies... my heart is going wild!]. in consistent manga style.'},\n",
       "  {'panel_number': 3,\n",
       "   'image_prompt': \"Comedic wide shot: IBAD_MNG (flustered, desperate) trips over feet, arms flailing, yelping 'G-Gyaa!' in sun-drenched courtyard; AISHA_MNG (curious, amused) watches from background, inner thought: [Oh no, oh no, oh no!]. in consistent manga style.\"},\n",
       "  {'panel_number': 4,\n",
       "   'image_prompt': 'Dynamic shot: IBAD_MNG (frantic, retreating) bolts away in comical dust cloud WHOOSH, leaving motion lines in sun-drenched courtyard. AISHA_MNG (bewildered, gentle smile) watches, inner thought: [Love just delivered a devastating, adorable SMASH!]. SMASH! in consistent manga style.'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state[\"manga_image_prompts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cdcee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
